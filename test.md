# 3장 평가 방법론

## 파운데이션 모델 평가의 과제

- 파운데이션 모델 평가가 어려운 이유 <br>
  (1) AI 모델이 똑똑해질수록 평가가 더 어려워짐
    - 초등학교 수학 문제 풀이는 누구나 틀렸는지 알 수 있다. 박사 수준의 문제 풀이라면? <br>
	(2) 파운데이션 모델은 개방형 모델이다
    - 전통적인 ML : 폐쇄형 모델. 분류 모델의 경우 출력 가능한 응답의 가짓수가 정해져 있다
    - 파운데이션 모델 : 개방형 모델. 올바른 응답이 너무 많다 <br>
  (3) "블랙박스"
    - 파운데이션 모델의 상세한 구조는 여러 이유로 인해 파악이 불가능함(개발자가 비공개, 사용자의 지식 부족 등) <br>
  (4) 모델을 평가할 벤치 마크 구성도 쉽지 않다
    - 파운데이션 모델의 발전 속도가 너무 빠름 : 좋은 평가 벤치 마크를 구성해도 수년 내에 포화 상태에 도달 <br>
  (5) 파운데이션 모델의 범용성
    - 하나의 모델이 이제 너무 많은 작업을 할 수 있다 : 평가 범위의 확장
      
- AI 분야에서 "평가" 에 대한 관심은 비약적으로 증가하였으나 여전히 다른 분야에 비하면 턱없이 부족함

![image](https://github.com/user-attachments/assets/f4296b30-c028-4f09-b755-6def7e1224ce)

## 언어 모델링 지표 이해

- 파운데이션 모델은 언어 모델을 기반으로 발전하였기 때문에 언어 모델에 대한 평가 지표는 여전히 유용하게 사용 가능함

### Entropy

- 엔트로피(Entropy)는 토큰이 평균적으로 얼마나 많은 정보를 전달하는지를 측정
- 아래 그림에서 정사각형 내의 위치를 설명하고자 함
- (a)와 같이 두 개의 토큰만 있는 경우 위치는 위/아래 2개 토큰 뿐. 토큰이 두 개뿐이므로 이를 나타내는 데 1비트면 충분함
- (b)와 같이 네 개의 토큰이 있는 경우 위치를 표현하기 위해 2비트가 필요
- 직관적으로 엔트로피는 언어에서 다음에 오는 것을 예측하기가 얼마나 어려운지를 측정

![image](https://github.com/user-attachments/assets/08565c98-2a79-4ad1-a308-7bd7f027be1e)

### Cross Entropy

- 데이터 세트에서 언어 모델을 훈련할 때 목표는 모델이 이 훈련 데이터의 분포를 학습하도록 하는 것
- 데이터 세트에서 언어 모델의 교차 엔트로피는 언어 모델이 이 데이터 세트에서 다음에 오는 것을 예측하기가 얼마나 어려운지를 측정
- 학습 데이터에 대한 모델의 교차 엔트로피는 두 가지 품질에 따라 달라집니다.
  - 학습 데이터의 엔트로피로 측정되는 학습 데이터의 예측 가능성
  - 언어 모델에 의해 포착된 분포가 학습 데이터의 실제 분포와 얼마나 다른지
- 따라서 Cross Entropy는 다음과 같다. $H(P, Q) = H(P) + D_{KL}(P || Q)$ .
- 언어 모델이 학습 데이터를 완벽하게 학습할 경우 교차 엔트로피는 학습 데이터의 엔트로피 $H(P)$와 정확히 같아진다

### BPC/BPB(문자당 비트 및 바이트당 비트)

- 언어 모델의 교차 엔트로피가 6비트이면 이 언어 모델은 각 토큰을 나타내는 데 6비트가 필요
- 서로 다른 모델은 서로 다른 토큰화 방법을 사용하므로 토큰당 비트 수는 모델 간에 비교할 수 없음
- 이를 보완하고자 한 지표가 BPC : 토큰당 비트 수가 6이고 평균적으로 각 토큰이 2개의 문자로 구성되면 BPC는 6/2 = 3
- BPC는 문자 인코딩 방식이 다를 경우 비교 불가 : ASCII에서는 각 문자가 7비트를 사용하여 인코딩, UTF-8에서는 문자가 8비트에서 32비트 사이를 사용하여 인코딩
- 이를 보완하고자 한 지표가 BPB : 언어 모델이 학습 데이터의 1바이트를 나타내는 데 필요한 비트 수. BPC가 3이고 각 문자가 7비트 또는 7/8바이트이면 BPB는 3 / (7/8) = 3.43입니다.

### 퍼플렉시티

- 실제 분포 P를 가진 데이터 세트가 주어지면 그 퍼플렉시티는 다음과 같이 정의됨 : $*PPL* (*P*) =  $2^{H(P)}$
- 교차 엔트로피가 모델이 다음 토큰을 예측하기 얼마나 어려운지를 측정한다면, 퍼플렉시티는 다음 토큰을 예측할 때 갖는 불확실성의 양을 측정
- 위 그림의 (b)에서 모델의 교차 엔트로피는 2비트. 이 때 정사각형의 위치를 예측한다면 4개의 가능한 옵션 중에서 선택해야함. 따라서 이 언어 모델의 퍼플렉시티는 4.
- 예시에선 엔트로피의 단위로 비트를 사용했으나 자연상수(e)도 많이 사용함

### 퍼플렉시티 해석 및 사용 사례

- 더 구조화된 데이터는 퍼플렉시티가 작다 : 데이터가 구조화될수록 예측이 더 쉽다
- 어휘(vocabulary)가 많을수록 퍼플렉시티가 크다 : 가능한 토큰이 많을수록 예측이 더 어렵다
- 컨텍스트 길이가 길수록 퍼플렉시티가 낮다 : 모델이 더 많은 컨텍스트를 가질수록 다음 토큰 예측에 대한 불확실성이 줄어든다

> [!CAUTION]
>
> 퍼플렉시티는 SFT 및 RLHF와 같은 기술을 사용하여 사후 훈련된(Post-trained) 모델을 평가하는 데 좋은 대리 변수가 아닐 수 있습니다.
> 사후 훈련은 모델에게 작업을 완료하는 방법을 가르치는 것입니다.
> 모델이 작업을 완료하는 데 능숙해짐에 따라 다음 토큰을 예측하는 데는 서툴러질 수 있습니다.
> 언어 모델의 퍼플렉시티는 일반적으로 사후 훈련 후 증가합니다.
> 어떤 사람들은 사후 훈련이 엔트로피를 *붕괴(collapses)*시킨다고 말합니다.
> 마찬가지로 모델의 숫자 정밀도와 메모리 공간을 줄이는 기술인 양자화(Quantization)도 모델의 퍼플렉시티를 예상치 못한 방식으로 변경할 수 있습니다.

- 텍스트에 대한 모델의 퍼플렉시티는 이 모델이 이 텍스트를 예측하기 얼마나 어려운지를 측정. 주어진 모델에 대해 퍼플렉시티는 모델이 훈련 중에 보고 기억한 텍스트에 대해 가장 낮다.
- 따라서 퍼플렉시티는 텍스트가 모델의 훈련 데이터에 있었는지 여부를 감지하는 데 사용할 수 있으며, 이는 데이터 오염을 감지하는 데 유용.
- 벤치마크 데이터에 대한 모델의 퍼플렉시티가 낮으면 이 벤치마크가 모델의 훈련 데이터에 포함되었을 가능성이 높으므로 이 벤치마크에 대한 모델의 성능은 덜 신뢰할 수 있다.
- 이는 또한 훈련 데이터의 중복 제거에도 사용할 수 있음 : 새 데이터의 퍼플렉시티가 높은 경우에만 기존 훈련 데이터 세트에 새 데이터를 추가
- 퍼플렉시티는 예측 불가능한 텍스트, 예를 들어 특이한 아이디어를 표현하는 텍스트("내 개는 여가 시간에 양자 물리학을 가르친다")나 횡설수설("집 고양이 가 눈")에 대해 가장 높아 비정상적인 텍스트 감지에 사용될 수 있다.
- 아래와 같이 텍스트에 대한 모델의 퍼플렉시티를 측정 가능

![image](https://github.com/user-attachments/assets/015aae19-bed0-4659-956b-282fe50de835)

## 정확한 평가(Exact Evaluation)

### 기능적 정확성(Functional Correctness)

- 기능적 정확성 평가는 시스템이 의도된 기능을 수행하는지 여부에 따라 시스템을 평가하는 것을 의미
- 측정 가능한 목표가 있는 작업은 일반적으로 기능적 정확성을 사용하여 평가할 수 있음
- 기능적 정확성 측정을 자동화할 수 있는 작업의 예시로 코딩이 대표적
- 두 숫자 num1과 num2의 최대 공약수(gcd)를 찾는 Python 함수 gcd(num1, num2)를 작성하도록 모델에 요청한다면, 테스트를 통해 올바른 답을 도출하는지 확인이 가능함
- 코드의 기능적 정확성을 자동으로 검증하는 것은 소프트웨어 엔지니어링에서 이미 오래된 표준 관행으로, 기능적 정확성을 평가하는 다양한 벤치 마크가 존재
- 벤치 마크 문제에는 아래와 같이 테스트 케이스가 함께 제공됨

![image](https://github.com/user-attachments/assets/1c896ec8-e746-4732-a770-0efed24f9815)

- 모델을 평가할 때 각 문제에 대해 k로 표시되는 여러 코드 샘플이 생성됨
- 모델이 생성한 k개 코드 샘플 중 하나라도 해당 문제의 모든 테스트 케이스를 통과하면 모델은 문제를 해결한 것으로 간주
- 최종 점수인 pass@k는 전체 문제 중에서 해결된 문제의 비율
- 10개의 문제가 있고 모델이 k=3으로 5개를 해결하면 해당 모델의 pass@3 점수는 50%
- 모델이 더 많은 코드 샘플을 생성할수록 각 문제를 해결할 기회가 더 많아지므로 최종 점수가 더 커짐(pass@1 점수가 pass@3보다 낮고, 이는 다시 pass@10보다 낮아야 함)

### 참조 데이터에 대한 유사성 측정(Similarity Measurements Against Reference Data)

- 기능적 정확성을 사용하여 자동으로 평가될 수 없는 경우 일반적인 접근 방식은 AI의 출력을 참조 데이터와 비교하여 평가
- 참조 데이터는 (입력, 참조 응답) 으로 구성되며, 하나의 입력에는 여러 가능한 참조 응답(reference responses)가 존재할 수 있음
- 참조 응답은 정답(ground truths) 또는 표준 응답(canonical responses)이라고도 함
- 참조가 필요한 지표는 참조 기반(reference-based)이고 참조가 필요하지 않은 지표는 참조 없음(reference-free)
- 이 평가 접근 방식은 참조 데이터를 필요로 하므로 참조 데이터를 얼마나 많이, 얼마나 빠르게 생성할 수 있는지에 따라 병목 현상이 발생
- 참조 데이터는 인간이 생성한 것과 AI가 생성한 것을 사용할 수 있다
- 참조 응답과 더 유사한 생성된 응답이 더 좋다고 간주되며, 두 개의 개방형 텍스트 간의 유사성을 측정하는 네 가지 방법이 존재
  - 평가자에게 두 텍스트가 동일한지 판단하도록 요청 : 인간 평가자, AI 평가자
  - 정확한 일치: 생성된 응답이 참조 응답 중 하나와 정확히 일치하는지 여부
  - 어휘적 유사성(Lexical similarity): 생성된 응답이 참조 응답과 얼마나 유사하게 보이는지
  - 의미론적 유사성(Semantic similarity): 생성된 응답이 의미(의미론)에서 참조 응답과 얼마나 가까운지

#### 정확한 일치

- 정확한 일치는 간단한 수학 문제, 일반적인 지식 쿼리, 퀴즈 스타일 질문과 같이 짧고 정확한 응답을 기대하는 작업에 효과적
- 간단한 작업 외에는 정확한 일치가 거의 작동하지 않음
  - 프랑스어 문장 "Comment ça va?"에 대해 "How are you?", "How is everything?", "How are you doing?" 세 가지 번역만 참조 데이터로 존재함
  - 모델이 "How is it going?"을 생성하면 모델의 응답은 틀린 것으로 표시됨

#### 어휘적 유사성(Lexical similarity)

- 어휘적 유사성은 두 텍스트가 얼마나 겹치는지 측정
- 가장 간단한 형태로 어휘적 유사성은 두 텍스트가 공통으로 가지는 토큰 수를 세어 측정
  - 참조 응답이 "My cats scare the mice" 라면
  - "My cats eat the mice"의 유사도는 80%(5개 단어 중 4개 포함)
  - "Cats and mice fight all the time"의 유사도는 60%(5개 단어 중 3개 포함)
- 또 다른 방법은 퍼지 매칭(fuzzy matching)으로 알려진 근사 문자열 매칭(approximate string matching)
  - 한 텍스트에서 다른 텍스트로 변환하는 데 필요한 편집 횟수, 즉 편집 거리(edit distance)라는 숫자를 세어 두 텍스트 간의 유사성을 측정하며, 일반적으로 아래 3가지 편집이 존재
      - 삭제: "brad" -> "bad"
      - 삽입: "bad" -> "bard"
      - 대체: "bad" -> "bed"
- 어휘적 유사성을 측정하는 또 다른 방법은 단일 토큰 대신 토큰 시퀀스인 n-그램(n-grams)의 중복을 기준으로 측정하는 n-그램 유사성(n-gram similarity)
  - 1-그램(유니그램)은 토큰, 2-그램(바이그램)은 두 토큰의 집합
  - "My cats scare the mice"는 "my cats", "cats scare", "scare the", "the mice"의 네 가지 바이그램으로 구성
  - 참조 응답의 n-그램 중 몇 퍼센트가 생성된 응답에도 있는지 측정
 
- 어휘적 유사성에 대한 일반적인 지표는 BLEU, ROUGE, METEOR++, TER, CIDEr이며. 중복 계산 방식에 따라 다름
- 파운데이션 모델이 부상한 이후로는 어휘적 유사성을 사용하는 벤치마크가 감소함
- 어휘적 유사성의 단점은 참조 응답 세트를 만들어야 한다는 것 : 참조 세트에 유사한 응답이 포함되어 있지 않으면 좋은 응답이 낮은 유사도 점수를 받을 수 있음
- 또한 참조 응답이 틀릴 수도 있다
- 어휘적 유사성 점수가 높다고 해서 항상 더 나은 응답을 의미하지는 않는다 : 코드 생성 벤치마크인 HumanEval에서 OpenAI는 잘못된 솔루션과 올바른 솔루션에 대한 BLEU 점수가 유사하다는 것을 발견

#### 의미론적 유사성(Semantic similarity)

- 어휘적 유사성은 두 텍스트가 비슷하게 보이는지를 측정하지만, 동일한 의미를 갖는지는 측정하지 않음 : "What's up?" vs "How are you?"
- 의미론적 유사성은 의미론의 유사성을 계산하는 것을 목표로 함
- 이를 위해서는 먼저 텍스트를 임베딩(embedding)이라는 숫자 표현으로 변환해야함. 따라서 의미론적 유사성은 임베딩 유사성(embedding similarity)이라고도 한다.
- 서로 다른 임베딩 알고리즘이 서로 다른 임베딩을 생성할 수 있으므로 의미론적 유사성에 의한 평가는 주관적으로 간주될 수 있음
- 임베딩으로 변환만 가능하다면 두 임베딩 벡터 사이의 유사도는 코사인 유사도 등으로 계산이 가능함
- 의미론적 유사성의 신뢰성은 기본 임베딩 알고리즘의 품질에 따라 달라지며, 기본 임베딩 알고리즘을 실행하는 데 상당한 컴퓨팅과 시간이 필요할 수 있음

### 임베딩 소개

- 컴퓨터는 숫자로 작동하므로 모델은 입력을 컴퓨터가 처리할 수 있는 숫자 표현으로 변환해야 함. 임베딩(embedding)은 원본 데이터의 의미를 포착하는 것을 목표로 하는 숫자 표현
- 특히 임베딩을 생성하도록 훈련된 모델에는 오픈 소스 모델 BERT, CLIP(대조적 언어-이미지 사전 훈련), Sentence Transformers 등이 있음
- GPT 및 Llama를 포함한 다른 모델들 역시 입력 데이터를 벡터로 변환하므로 임베딩에 활용할 수 있으나 특수 임베딩 모델에 비해 좋지 못할 수 있다

![image](https://github.com/user-attachments/assets/e5bee050-b844-4907-88b2-0e1cb8a3f1a2)

- 임베딩 알고리즘의 목표는 원본 데이터의 본질을 포착하는 임베딩을 생성하는 것. 그렇다면 임베딩이 잘 되었는지 검증하는 방법은?
  - 더 유사한 텍스트가 코사인 유사도 또는 관련 지표로 측정했을 때 더 가까운 임베딩을 가지면 임베딩 알고리즘이 좋다고 간주
  - 작업에 대한 유용성을 기준으로 임베딩 품질을 평가할 수 있음. 임베딩은 분류, 토픽 모델링, 추천 시스템, RAG 등 많은 작업에 사용됨
  - 여러 작업에서 임베딩 품질을 측정하는 벤치마크의 예로는 MTEB(대규모 텍스트 임베딩 벤치마크)(Muennigh‐off et al., 2023)가 있다

- 텍스트 뿐만 아니라 모든 형태의 데이터가 임베딩으로 표현 가능
- 궁극적인 목표는 서로 다른 모달리티의 데이터에 대한 공동 임베딩을 만드는 것
  - CLIP(Radford et al., 2021) : 텍스트와 이미지라는 서로 다른 모달리티의 데이터를 공동 임베딩 공간에 매핑할 수 있는 최초의 주요 모델
  - ULIP(언어, 이미지, 포인트 클라우드의 통합 표현)(Xue et al., 2022) : 텍스트, 이미지, 3D 포인트 클라우드의 통합 표현을 만드는 것이 목표
  - ImageBind(Girdhar et al., 2023) : 텍스트, 이미지, 오디오를 포함한 6가지 서로 다른 모달리티에 걸쳐 공동 임베딩을 학습
- 아래 예시는 CLIP의 도식화
  - CLIP은 (이미지, 텍스트) 쌍을 사용하여 훈련되며, 이미지에 해당하는 텍스트는 캡션이거나 이 이미지와 관련된 주석일 수 있음
  - 각 (이미지, 텍스트) 쌍에 대해 CLIP은 텍스트 인코더를 사용하여 텍스트를 텍스트 임베딩으로 변환하고 이미지 인코더를 사용하여 이미지를 이미지 임베딩으로 변환하고 임베딩을 모두 공동 임베딩 공간으로 투영(projection)
  - 모델의 학습 목표는 이 공동 공간에서 이미지의 임베딩을 해당 텍스트의 임베딩에 가깝게 만드는 것

![image](https://github.com/user-attachments/assets/d931431c-5d7d-4275-9b0a-d8e973c25ab7)

- 서로 다른 모달리티의 데이터를 나타낼 수 있는 공동 임베딩 공간은 멀티모달 임베딩 공간(multimodal embedding space)
- 텍스트-이미지 공동 임베딩 공간에서 낚시하는 남자의 이미지 임베딩은 "패션쇼"라는 텍스트의 임베딩보다 "어부"라는 텍스트의 임베딩에 더 가까워야 함
- 이 공동 임베딩 공간은 서로 다른 모달리티의 임베딩을 비교하고 결합할 수 있도록 하며, 이는 텍스트 기반 이미지 검색을 가능하게 함

## 심사위원으로서의 AI

- 개방형 모델의 응답 평가에 대한 어려움으로 인해 많은 팀이 인간 평가에 의존
- AI를 사용하여 평가를 자동화한다는 아이디어는 오랫동안 존재해 왔지만, 가능성이 보인 것은 GPT-3가 출시된 2020년경
- 현재, AI 심사위원은 운영 상에서 AI 모델을 평가하는 가장 일반적인 방법 중 하나가 됨
- LangChain의 2023년 AI 현황 보고서에 따르면 플랫폼 평가의 58%가 AI 심사위원에 의해 수행

### 왜 AI를 심사위원으로 사용하는가?

- AI 심사위원은 인간 평가자에 비해 빠르고 사용하기 쉬우며 상대적으로 저렴함. 또한 참조 데이터 없이도 작동할 수 있으므로 참조 데이터가 없는 운영 환경에서 사용 가능
- AI 모델에게 정확성, 반복성, 유해성, 건전성, 환각 등 모든 기준에 따라 출력을 판단하도록 요청할 수 있음
- 사람의 판단을 항상 신뢰할 수 없듯이 AI의 판단을 항상 신뢰할 수도 없다
- 그러나 각 AI 모델은 대중의 집합체이므로 AI 모델이 대중을 대표하는 판단을 내릴 수 있으며, 올바른 모델에 대한 올바른 프롬프트를 사용하면 광범위한 주제에 대해 합리적으로 좋은 판단을 얻을 수 있다.
- 연구에 따르면 특정 AI 심사위원은 인간 평가자와 밀접하게 관련되어 있음
  - 2023년 Zheng et al.은 평가 벤치마크인 MT-Bench에서 GPT-4와 인간의 일치율이 85%에 도달하여 인간 간의 일치율(81%)보다 훨씬 높다는 것을 발견
  - AlpacaEval 작성자(Dubois et al., 2023)도 AI 심사위원이 인간이 평가하는 LMSYS의 Chat Arena 리더보드와 거의 완벽한(0.98) 상관 관계를 갖는다는 것을 발견함
- 유연성 덕분에 AI는 심사위원으로서 광범위한 애플리케이션에 유용하며, 일부 애플리케이션에서는 유일한 자동 평가 옵션임
- AI는 응답을 평가할 수 있을 뿐만 아니라 결정을 설명할 수도 있으며, 이는 평가 결과를 감사하려는 경우 특히 유용할 수 있다. 아래 그림 3-7은 GPT-4가 판단을 설명하는 예시이다.

![image](https://github.com/user-attachments/assets/4f1d9b01-507b-47d6-b065-07bbc7548d17)

### AI를 심사위원으로 사용하는 방법

- AI를 사용하여 판단을 내리는 방법에는 여러 가지가 있다.
- 예를 들어, AI를 사용하여 응답 자체의 품질을 평가하거나, 해당 응답을 참조 데이터와 비교하거나, 해당 응답을 다른 응답과 비교할 수 있다.
- 위 세 가지 접근 방식에 대한 간단한 프롬프트 예시

![image](https://github.com/user-attachments/assets/14dc305b-4e5a-478a-bcb2-de1586307f65)

- 범용 AI 심사위원에게 어떤 기준에 따라 응답을 평가하도록 요청할 수 있음
- 역할극 챗봇을 구축하는 경우 챗봇의 응답이 사용자가 원하는 역할과 일치하는지 평가 가능
  - 홍보용 제품 사진을 생성하는 애플리케이션을 구축하는 경우 "이 이미지의 제품 신뢰도를 1점에서 5점까지 어떻게 평가하시겠습니까?"라고 물을 수 있음
- AI 심사위원 기준은 표준화되어 있지 않다
  - Azure AI Studio의 점수는 MLflow의 점수와 매우 다를 수 있으며, 이러한 점수는 심사위원의 기본 모델과 프롬프트에 따라 달라짐

![image](https://github.com/user-attachments/assets/2202f5a2-4a06-4ba2-9c6f-4b33a4841265)

- AI 심사위원에게 프롬프트를 제공하는 방법은 모든 AI 애플리케이션에 프롬프트를 제공하는 방법과 유사하며, 일반적으로 심사위원의 프롬프트는 다음을 명확하게 설명해야 함
  - 모델이 수행할 작업(예: 생성된 답변과 질문 간의 관련성 평가).
  - 모델이 평가를 위해 따라야 할 기준(예: "최우선 사항은 생성된 답변이 정답에 따라 주어진 질문을 해결하기에 충분한 정보를 포함하는지 여부를 결정하는 것이어야 한다"). 지침이 자세할수록 좋다.
  - 채점 시스템은 다음 중 하나일 수 있음
    - 분류(예: 좋음/나쁨 또는 관련 있음/관련 없음/중립).
    - 이산적인 숫자 값(예: 1에서 5). 이산적인 숫자 값은 각 클래스가 의미론적 해석 대신 숫자 해석을 갖는 분류의 특별한 경우로 간주될 수 있음
    - 연속적인 숫자 값(예: 0과 1 사이, 유사성 정도를 평가하려는 경우).
- 예제가 포함된 프롬프트가 더 나은 성능을 보인다는 것이 입증됨. 1점에서 5점 사이의 채점 시스템을 사용하는 경우 1, 2, 3, 4 또는 5점 응답이 어떤 모습인지, 가능하다면 특정 점수를 받은 이유에 대한 예를 포함하는 것이 좋다

> [!NOTE]
>
> 언어 모델은 일반적으로 숫자보다 텍스트에 더 능숙하여 AI 심사위원은 숫자 채점 시스템보다 분류에서 더 잘 작동한다고 보고됨.
> 숫자 채점 시스템의 경우 이산 채점이 연속 채점보다 더 잘 작동하는 것으로 보임. 경험적으로 이산 채점 범위가 넓을수록 모델 성능이 저하되는 경향이 있으며, 일반적인 이산 채점 시스템은 1에서 5 사이가 권장됨

![image](https://github.com/user-attachments/assets/23e39bc7-28ee-481e-8836-d57e9edf7d68)


> [!NOTE]
>
> - 아래는 Azure AI Studio에서 관련성 기준에 사용된 프롬프트의 일부임
> 귀하의 임무는 생성된 답변과 질문 간의 관련성을 정답을 기준으로 1점에서 5점 사이의 범위로 채점하고 채점 이유도 제공하는 것입니다.
> 주요 초점은 생성된 답변이 정답에 따라 주어진 질문을 해결하기에 충분한 정보를 포함하는지 여부를 결정하는 것이어야 합니다.
> ...
> 생성된 답변이 정답과 모순되면 낮은 점수(1-2점)를 받습니다.
> 예를 들어, "하늘이 파란가요?"라는 질문에 대한 정답은 "예, 하늘은 파랗습니다."이고 생성된 답변은 "아니요, 하늘은 파랗지 않습니다."입니다.
> 이 예에서 생성된 답변은 하늘이 파랗다는 사실에도 불구하고 하늘이 파랗지 않다고 진술함으로써 정답과 모순됩니다.
> 이러한 불일치는 낮은 점수(1-2점)를 초래하며 낮은 점수의 이유는 생성된 답변과 정답 간의 모순을 반영합니다.


### AI 심사위원의 한계

#### 불일치(Inconsistency)

- 평가 방법이 신뢰할 수 있으려면 결과가 일관되어야 함
- 그러나 모든 AI 애플리케이션과 마찬가지로 AI 심사위원도 확률적이며, 동일한 심사위원이 동일한 입력에 대해 프롬프트가 다르면 다른 점수를 출력할 수 있음
- 동일한 심사위원이라도 동일한 지침으로 프롬프트해도 두 번 실행하면 다른 점수를 출력할 수도 있으며, 이러한 불일치로 인해 평가 결과를 재현하거나 신뢰하기 어려움
- 2장에서 설명한 샘플링 변수를 사용하여 AI 심사위원을 더 일관되게 만들 수 있음
  - Zheng et al.(2023)은 프롬프트에 평가 예제를 포함하면 GPT-4의 일관성을 65%에서 77.5%로 높일 수 있음을 보여주었다.
  - 그러나 높은 일관성이 높은 정확성을 의미하지 않을 수 있다 : 심사위원이 일관되게 동일한 실수를 저지를 수 있음
  - 또한 더 많은 예제를 포함하면 프롬프트가 길어지고 긴 프롬프트는 추론 비용이 높아짐을 의미함 : Zheng et al.의 실험에서 프롬프트에 더 많은 예제를 포함하자 GPT-4 지출이 4배 증가

#### 기준 모호성(Criteria ambiguity)

- 인간이 만든 많은 지표와 달리 AI 심사위원 지표는 표준화되어 있지 않아 오해하고 오용하기 쉬움
- MLflow, Ragas, LlamaIndex는 모두 생성된 출력이 주어진 컨텍스트에 얼마나 충실한지를 측정하기 위한 기본 제공 기준 충실도(faithfulness)를 가지고 있지만 지침과 채점 시스템은 모두 다름

| 도구   | 프롬프트<br>[간결성을 위해 일부 생략]                                                                                                                                                                                                                                                                                                                           | 채점<br>시스템 |
| ------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------- |
| MLflow | 충실도는 제공된 출력과 제공된 컨텍스트로만 평가되며, 충실도를 채점할 때 제공된 입력을 완전히 무시하십시오. 충실도는 제공된 출력의 어느 정도가 제공된 컨텍스트와 사실적으로 일치하는지를 평가합니다....<br><br>충실도: 다음은 다양한 점수에 대한 세부 정보입니다.<br>- 점수 1: 출력의 주장 중 어느 것도 제공된 컨텍스트에서 추론할 수 없습니다.<br>- 점수 2: ... | 1–5            |
| Ragas  | 귀하의 임무는 주어진 컨텍스트를 기반으로 일련의 진술의 충실도를 판단하는 것입니다. 각 진술에 대해 컨텍스트를 기반으로 진술을 확인할 수 있으면 1로, 컨텍스트를 기반으로 진술을 확인할 수 없으면 0으로 평결을 반환해야 합니다.                                                                                                                                    | 0과 1          |
| LlamaIndex | 주어진 정보가 컨텍스트에 의해 지원되는지<br>알려주십시오.<br>YES 또는 NO로 답변해야 합니다.<br>대부분의 컨텍스트가 관련이 없더라도<br>컨텍스트 중 하나라도 정보를 지원하면 YES로 답변하십시오.<br>몇 가지 예가 아래에 제공됩니다.<br>정보: 사과 파이는 일반적으로 이중 껍질입니다.<br>컨텍스트: 사과 파이는 과일 파이입니다… 일반적으로 이중<br>껍질이며,<br>페이스트리가<br>필링 위아래에 모두 있습니다.<br>답변: YES | YES와<br>NO    |



