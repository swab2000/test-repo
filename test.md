# 3장 평가 방법론

## 파운데이션 모델 평가의 과제

- 파운데이션 모델 평가가 어려운 이유 <br>
  (1) AI 모델이 똑똑해질수록 평가가 더 어려워짐
    - 초등학교 수학 문제 풀이는 누구나 틀렸는지 알 수 있다. 박사 수준의 문제 풀이라면? <br>
	(2) 파운데이션 모델은 개방형 모델이다
    - 전통적인 ML : 폐쇄형 모델. 분류 모델의 경우 출력 가능한 응답의 가짓수가 정해져 있다
    - 파운데이션 모델 : 개방형 모델. 올바른 응답이 너무 많다 <br>
  (3) "블랙박스"
    - 파운데이션 모델의 상세한 구조는 여러 이유로 인해 파악이 불가능함(개발자가 비공개, 사용자의 지식 부족 등) <br>
  (4) 모델을 평가할 벤치 마크 구성도 쉽지 않다
    - 파운데이션 모델의 발전 속도가 너무 빠름 : 좋은 평가 벤치 마크를 구성해도 수년 내에 포화 상태에 도달 <br>
  (5) 파운데이션 모델의 범용성
    - 하나의 모델이 이제 너무 많은 작업을 할 수 있다 : 평가 범위의 확장
      
- AI 분야에서 "평가" 에 대한 관심은 비약적으로 증가하였으나 여전히 다른 분야에 비하면 턱없이 부족함

![image](https://github.com/user-attachments/assets/f4296b30-c028-4f09-b755-6def7e1224ce)

## 언어 모델링 지표 이해

- 파운데이션 모델은 언어 모델을 기반으로 발전하였기 때문에 언어 모델에 대한 평가 지표는 여전히 유용하게 사용 가능함

### Entropy

- 엔트로피(Entropy)는 토큰이 평균적으로 얼마나 많은 정보를 전달하는지를 측정
- 아래 그림에서 정사각형 내의 위치를 설명하고자 함
- (a)와 같이 두 개의 토큰만 있는 경우 위치는 위/아래 2개 토큰 뿐. 토큰이 두 개뿐이므로 이를 나타내는 데 1비트면 충분함
- (b)와 같이 네 개의 토큰이 있는 경우 위치를 표현하기 위해 2비트가 필요
- 직관적으로 엔트로피는 언어에서 다음에 오는 것을 예측하기가 얼마나 어려운지를 측정

![image](https://github.com/user-attachments/assets/08565c98-2a79-4ad1-a308-7bd7f027be1e)

### Cross Entropy

- 데이터 세트에서 언어 모델을 훈련할 때 목표는 모델이 이 훈련 데이터의 분포를 학습하도록 하는 것
- 데이터 세트에서 언어 모델의 교차 엔트로피는 언어 모델이 이 데이터 세트에서 다음에 오는 것을 예측하기가 얼마나 어려운지를 측정
- 학습 데이터에 대한 모델의 교차 엔트로피는 두 가지 품질에 따라 달라집니다.
  - 학습 데이터의 엔트로피로 측정되는 학습 데이터의 예측 가능성
  - 언어 모델에 의해 포착된 분포가 학습 데이터의 실제 분포와 얼마나 다른지
- 따라서 Cross Entropy는 다음과 같다. $H(P, Q) = H(P) + D_{KL}(P || Q)$ .
- 언어 모델이 학습 데이터를 완벽하게 학습할 경우 교차 엔트로피는 학습 데이터의 엔트로피 $H(P)$와 정확히 같아진다

### BPC/BPB(문자당 비트 및 바이트당 비트)

- 언어 모델의 교차 엔트로피가 6비트이면 이 언어 모델은 각 토큰을 나타내는 데 6비트가 필요
- 서로 다른 모델은 서로 다른 토큰화 방법을 사용하므로 토큰당 비트 수는 모델 간에 비교할 수 없음
- 이를 보완하고자 한 지표가 BPC : 토큰당 비트 수가 6이고 평균적으로 각 토큰이 2개의 문자로 구성되면 BPC는 6/2 = 3
- BPC는 문자 인코딩 방식이 다를 경우 비교 불가 : ASCII에서는 각 문자가 7비트를 사용하여 인코딩, UTF-8에서는 문자가 8비트에서 32비트 사이를 사용하여 인코딩
- 이를 보완하고자 한 지표가 BPB : 언어 모델이 학습 데이터의 1바이트를 나타내는 데 필요한 비트 수. BPC가 3이고 각 문자가 7비트 또는 7/8바이트이면 BPB는 3 / (7/8) = 3.43입니다.

### 퍼플렉시티

- 실제 분포 P를 가진 데이터 세트가 주어지면 그 퍼플렉시티는 다음과 같이 정의됨 : $*PPL* (*P*) =  $2^{H(P)}$
- 교차 엔트로피가 모델이 다음 토큰을 예측하기 얼마나 어려운지를 측정한다면, 퍼플렉시티는 다음 토큰을 예측할 때 갖는 불확실성의 양을 측정
- 위 그림의 (b)에서 모델의 교차 엔트로피는 2비트. 이 때 정사각형의 위치를 예측한다면 4개의 가능한 옵션 중에서 선택해야함. 따라서 이 언어 모델의 퍼플렉시티는 4.
- 예시에선 엔트로피의 단위로 비트를 사용했으나 자연상수(e)도 많이 사용함

### 퍼플렉시티 해석 및 사용 사례

- 더 구조화된 데이터는 퍼플렉시티가 작다 : 데이터가 구조화될수록 예측이 더 쉽다
- 어휘(vocabulary)가 많을수록 퍼플렉시티가 크다 : 가능한 토큰이 많을수록 예측이 더 어렵다
- 컨텍스트 길이가 길수록 퍼플렉시티가 낮다 : 모델이 더 많은 컨텍스트를 가질수록 다음 토큰 예측에 대한 불확실성이 줄어든다

> [!CAUTION]
>
> 퍼플렉시티는 SFT 및 RLHF와 같은 기술을 사용하여 사후 훈련된(Post-trained) 모델을 평가하는 데 좋은 대리 변수가 아닐 수 있습니다.
> 사후 훈련은 모델에게 작업을 완료하는 방법을 가르치는 것입니다.
> 모델이 작업을 완료하는 데 능숙해짐에 따라 다음 토큰을 예측하는 데는 서툴러질 수 있습니다.
> 언어 모델의 퍼플렉시티는 일반적으로 사후 훈련 후 증가합니다.
> 어떤 사람들은 사후 훈련이 엔트로피를 *붕괴(collapses)*시킨다고 말합니다.
> 마찬가지로 모델의 숫자 정밀도와 메모리 공간을 줄이는 기술인 양자화(Quantization)도 모델의 퍼플렉시티를 예상치 못한 방식으로 변경할 수 있습니다.

- 텍스트에 대한 모델의 퍼플렉시티는 이 모델이 이 텍스트를 예측하기 얼마나 어려운지를 측정. 주어진 모델에 대해 퍼플렉시티는 모델이 훈련 중에 보고 기억한 텍스트에 대해 가장 낮다.
- 따라서 퍼플렉시티는 텍스트가 모델의 훈련 데이터에 있었는지 여부를 감지하는 데 사용할 수 있으며, 이는 데이터 오염을 감지하는 데 유용.
- 벤치마크 데이터에 대한 모델의 퍼플렉시티가 낮으면 이 벤치마크가 모델의 훈련 데이터에 포함되었을 가능성이 높으므로 이 벤치마크에 대한 모델의 성능은 덜 신뢰할 수 있다.
- 이는 또한 훈련 데이터의 중복 제거에도 사용할 수 있음 : 새 데이터의 퍼플렉시티가 높은 경우에만 기존 훈련 데이터 세트에 새 데이터를 추가
- 퍼플렉시티는 예측 불가능한 텍스트, 예를 들어 특이한 아이디어를 표현하는 텍스트("내 개는 여가 시간에 양자 물리학을 가르친다")나 횡설수설("집 고양이 가 눈")에 대해 가장 높아 비정상적인 텍스트 감지에 사용될 수 있다.
- 아래와 같이 텍스트에 대한 모델의 퍼플렉시티를 측정 가능

![image](https://github.com/user-attachments/assets/015aae19-bed0-4659-956b-282fe50de835)

## 정확한 평가(Exact Evaluation)

### 기능적 정확성(Functional Correctness)

- 기능적 정확성 평가는 시스템이 의도된 기능을 수행하는지 여부에 따라 시스템을 평가하는 것을 의미
- 측정 가능한 목표가 있는 작업은 일반적으로 기능적 정확성을 사용하여 평가할 수 있음
- 기능적 정확성 측정을 자동화할 수 있는 작업의 예시로 코딩이 대표적
- 두 숫자 num1과 num2의 최대 공약수(gcd)를 찾는 Python 함수 gcd(num1, num2)를 작성하도록 모델에 요청한다면, 테스트를 통해 올바른 답을 도출하는지 확인이 가능함
- 코드의 기능적 정확성을 자동으로 검증하는 것은 소프트웨어 엔지니어링에서 이미 오래된 표준 관행으로, 기능적 정확성을 평가하는 다양한 벤치 마크가 존재
- 벤치 마크 문제에는 아래와 같이 테스트 케이스가 함께 제공됨

![image](https://github.com/user-attachments/assets/1c896ec8-e746-4732-a770-0efed24f9815)

- 모델을 평가할 때 각 문제에 대해 k로 표시되는 여러 코드 샘플이 생성됨
- 모델이 생성한 k개 코드 샘플 중 하나라도 해당 문제의 모든 테스트 케이스를 통과하면 모델은 문제를 해결한 것으로 간주
- 최종 점수인 pass@k는 전체 문제 중에서 해결된 문제의 비율
- 10개의 문제가 있고 모델이 k=3으로 5개를 해결하면 해당 모델의 pass@3 점수는 50%
- 모델이 더 많은 코드 샘플을 생성할수록 각 문제를 해결할 기회가 더 많아지므로 최종 점수가 더 커짐(pass@1 점수가 pass@3보다 낮고, 이는 다시 pass@10보다 낮아야 함)

### 참조 데이터에 대한 유사성 측정(Similarity Measurements Against Reference Data)

- 기능적 정확성을 사용하여 자동으로 평가될 수 없는 경우 일반적인 접근 방식은 AI의 출력을 참조 데이터와 비교하여 평가
- 참조 데이터는 (입력, 참조 응답) 으로 구성되며, 하나의 입력에는 여러 가능한 참조 응답(reference responses)가 존재할 수 있음
- 참조 응답은 정답(ground truths) 또는 표준 응답(canonical responses)이라고도 함
- 참조가 필요한 지표는 참조 기반(reference-based)이고 참조가 필요하지 않은 지표는 참조 없음(reference-free)
- 이 평가 접근 방식은 참조 데이터를 필요로 하므로 참조 데이터를 얼마나 많이, 얼마나 빠르게 생성할 수 있는지에 따라 병목 현상이 발생
- 참조 데이터는 인간이 생성한 것과 AI가 생성한 것을 사용할 수 있다
- 참조 응답과 더 유사한 생성된 응답이 더 좋다고 간주되며, 두 개의 개방형 텍스트 간의 유사성을 측정하는 네 가지 방법이 존재
  - 평가자에게 두 텍스트가 동일한지 판단하도록 요청 : 인간 평가자, AI 평가자
  - 정확한 일치: 생성된 응답이 참조 응답 중 하나와 정확히 일치하는지 여부
  - 어휘적 유사성(Lexical similarity): 생성된 응답이 참조 응답과 얼마나 유사하게 보이는지
  - 의미론적 유사성(Semantic similarity): 생성된 응답이 의미(의미론)에서 참조 응답과 얼마나 가까운지

#### 정확한 일치

- 정확한 일치는 간단한 수학 문제, 일반적인 지식 쿼리, 퀴즈 스타일 질문과 같이 짧고 정확한 응답을 기대하는 작업에 효과적
- 간단한 작업 외에는 정확한 일치가 거의 작동하지 않음
  - 프랑스어 문장 "Comment ça va?"에 대해 "How are you?", "How is everything?", "How are you doing?" 세 가지 번역만 참조 데이터로 존재함
  - 모델이 "How is it going?"을 생성하면 모델의 응답은 틀린 것으로 표시됨

#### 어휘적 유사성(Lexical similarity)

- 어휘적 유사성은 두 텍스트가 얼마나 겹치는지 측정
- 가장 간단한 형태로 어휘적 유사성은 두 텍스트가 공통으로 가지는 토큰 수를 세어 측정
  - 참조 응답이 "My cats scare the mice" 라면
  - "My cats eat the mice"의 유사도는 80%(5개 단어 중 4개 포함)
  - "Cats and mice fight all the time"의 유사도는 60%(5개 단어 중 3개 포함)
- 또 다른 방법은 퍼지 매칭(fuzzy matching)으로 알려진 근사 문자열 매칭(approximate string matching)
  - 한 텍스트에서 다른 텍스트로 변환하는 데 필요한 편집 횟수, 즉 편집 거리(edit distance)라는 숫자를 세어 두 텍스트 간의 유사성을 측정하며, 일반적으로 아래 3가지 편집이 존재
      - 삭제: "brad" -> "bad"
      - 삽입: "bad" -> "bard"
      - 대체: "bad" -> "bed"
- 어휘적 유사성을 측정하는 또 다른 방법은 단일 토큰 대신 토큰 시퀀스인 n-그램(n-grams)의 중복을 기준으로 측정하는 n-그램 유사성(n-gram similarity)
  - 1-그램(유니그램)은 토큰, 2-그램(바이그램)은 두 토큰의 집합
  - "My cats scare the mice"는 "my cats", "cats scare", "scare the", "the mice"의 네 가지 바이그램으로 구성
  - 참조 응답의 n-그램 중 몇 퍼센트가 생성된 응답에도 있는지 측정
 
- 어휘적 유사성에 대한 일반적인 지표는 BLEU, ROUGE, METEOR++, TER, CIDEr이며. 중복 계산 방식에 따라 다름
- 파운데이션 모델이 부상한 이후로는 어휘적 유사성을 사용하는 벤치마크가 감소함
- 어휘적 유사성의 단점은 참조 응답 세트를 만들어야 한다는 것 : 참조 세트에 유사한 응답이 포함되어 있지 않으면 좋은 응답이 낮은 유사도 점수를 받을 수 있음
- 또한 참조 응답이 틀릴 수도 있다
- 어휘적 유사성 점수가 높다고 해서 항상 더 나은 응답을 의미하지는 않는다 : 코드 생성 벤치마크인 HumanEval에서 OpenAI는 잘못된 솔루션과 올바른 솔루션에 대한 BLEU 점수가 유사하다는 것을 발견

#### 의미론적 유사성(Semantic similarity)

- 어휘적 유사성은 두 텍스트가 비슷하게 보이는지를 측정하지만, 동일한 의미를 갖는지는 측정하지 않음 : "What's up?" vs "How are you?"
- 의미론적 유사성은 의미론의 유사성을 계산하는 것을 목표로 함
- 이를 위해서는 먼저 텍스트를 임베딩(embedding)이라는 숫자 표현으로 변환해야함. 따라서 의미론적 유사성은 임베딩 유사성(embedding similarity)이라고도 한다.
- 서로 다른 임베딩 알고리즘이 서로 다른 임베딩을 생성할 수 있으므로 의미론적 유사성에 의한 평가는 주관적으로 간주될 수 있음
- 임베딩으로 변환만 가능하다면 두 임베딩 벡터 사이의 유사도는 코사인 유사도 등으로 계산이 가능함
- 의미론적 유사성의 신뢰성은 기본 임베딩 알고리즘의 품질에 따라 달라지며, 기본 임베딩 알고리즘을 실행하는 데 상당한 컴퓨팅과 시간이 필요할 수 있음

### 임베딩 소개

- 컴퓨터는 숫자로 작동하므로 모델은 입력을 컴퓨터가 처리할 수 있는 숫자 표현으로 변환해야 함. 임베딩(embedding)은 원본 데이터의 의미를 포착하는 것을 목표로 하는 숫자 표현
- 특히 임베딩을 생성하도록 훈련된 모델에는 오픈 소스 모델 BERT, CLIP(대조적 언어-이미지 사전 훈련), Sentence Transformers 등이 있음
- GPT 및 Llama를 포함한 다른 모델들 역시 입력 데이터를 벡터로 변환하므로 임베딩에 활용할 수 있으나 특수 임베딩 모델에 비해 좋지 못할 수 있다

![image](https://github.com/user-attachments/assets/e5bee050-b844-4907-88b2-0e1cb8a3f1a2)

- 임베딩 알고리즘의 목표는 원본 데이터의 본질을 포착하는 임베딩을 생성하는 것. 그렇다면 임베딩이 잘 되었는지 검증하는 방법은?
  - 더 유사한 텍스트가 코사인 유사도 또는 관련 지표로 측정했을 때 더 가까운 임베딩을 가지면 임베딩 알고리즘이 좋다고 간주
  - 작업에 대한 유용성을 기준으로 임베딩 품질을 평가할 수 있음. 임베딩은 분류, 토픽 모델링, 추천 시스템, RAG 등 많은 작업에 사용됨
  - 여러 작업에서 임베딩 품질을 측정하는 벤치마크의 예로는 MTEB(대규모 텍스트 임베딩 벤치마크)(Muennigh‐off et al., 2023)가 있다

- 텍스트 뿐만 아니라 모든 형태의 데이터가 임베딩으로 표현 가능
- 궁극적인 목표는 서로 다른 모달리티의 데이터에 대한 공동 임베딩을 만드는 것
  - CLIP(Radford et al., 2021) : 텍스트와 이미지라는 서로 다른 모달리티의 데이터를 공동 임베딩 공간에 매핑할 수 있는 최초의 주요 모델
  - ULIP(언어, 이미지, 포인트 클라우드의 통합 표현)(Xue et al., 2022) : 텍스트, 이미지, 3D 포인트 클라우드의 통합 표현을 만드는 것이 목표
  - ImageBind(Girdhar et al., 2023) : 텍스트, 이미지, 오디오를 포함한 6가지 서로 다른 모달리티에 걸쳐 공동 임베딩을 학습
- 아래 예시는 CLIP의 도식화
  - CLIP은 (이미지, 텍스트) 쌍을 사용하여 훈련되며, 이미지에 해당하는 텍스트는 캡션이거나 이 이미지와 관련된 주석일 수 있음
  - 각 (이미지, 텍스트) 쌍에 대해 CLIP은 텍스트 인코더를 사용하여 텍스트를 텍스트 임베딩으로 변환하고 이미지 인코더를 사용하여 이미지를 이미지 임베딩으로 변환하고 임베딩을 모두 공동 임베딩 공간으로 투영(projection)
  - 모델의 학습 목표는 이 공동 공간에서 이미지의 임베딩을 해당 텍스트의 임베딩에 가깝게 만드는 것

![image](https://github.com/user-attachments/assets/d931431c-5d7d-4275-9b0a-d8e973c25ab7)

- 서로 다른 모달리티의 데이터를 나타낼 수 있는 공동 임베딩 공간은 멀티모달 임베딩 공간(multimodal embedding space)
- 텍스트-이미지 공동 임베딩 공간에서 낚시하는 남자의 이미지 임베딩은 "패션쇼"라는 텍스트의 임베딩보다 "어부"라는 텍스트의 임베딩에 더 가까워야 함
- 이 공동 임베딩 공간은 서로 다른 모달리티의 임베딩을 비교하고 결합할 수 있도록 하며, 이는 텍스트 기반 이미지 검색을 가능하게 함

## 심사위원으로서의 AI

- 개방형 모델의 응답 평가에 대한 어려움으로 인해 많은 팀이 인간 평가에 의존
- AI를 사용하여 평가를 자동화한다는 아이디어는 오랫동안 존재해 왔지만, 가능성이 보인 것은 GPT-3가 출시된 2020년경
- 현재, AI 심사위원은 운영 상에서 AI 모델을 평가하는 가장 일반적인 방법 중 하나가 됨
- LangChain의 2023년 AI 현황 보고서에 따르면 플랫폼 평가의 58%가 AI 심사위원에 의해 수행

### 왜 AI를 심사위원으로 사용하는가?

- AI 심사위원은 인간 평가자에 비해 빠르고 사용하기 쉬우며 상대적으로 저렴함. 또한 참조 데이터 없이도 작동할 수 있으므로 참조 데이터가 없는 운영 환경에서 사용 가능
- AI 모델에게 정확성, 반복성, 유해성, 건전성, 환각 등 모든 기준에 따라 출력을 판단하도록 요청할 수 있음
- 사람의 판단을 항상 신뢰할 수 없듯이 AI의 판단을 항상 신뢰할 수도 없다
- 그러나 각 AI 모델은 대중의 집합체이므로 AI 모델이 대중을 대표하는 판단을 내릴 수 있으며, 올바른 모델에 대한 올바른 프롬프트를 사용하면 광범위한 주제에 대해 합리적으로 좋은 판단을 얻을 수 있다.
- 연구에 따르면 특정 AI 심사위원은 인간 평가자와 밀접하게 관련되어 있음
  - 2023년 Zheng et al.은 평가 벤치마크인 MT-Bench에서 GPT-4와 인간의 일치율이 85%에 도달하여 인간 간의 일치율(81%)보다 훨씬 높다는 것을 발견
  - AlpacaEval 작성자(Dubois et al., 2023)도 AI 심사위원이 인간이 평가하는 LMSYS의 Chat Arena 리더보드와 거의 완벽한(0.98) 상관 관계를 갖는다는 것을 발견함
- 유연성 덕분에 AI는 심사위원으로서 광범위한 애플리케이션에 유용하며, 일부 애플리케이션에서는 유일한 자동 평가 옵션임
- AI는 응답을 평가할 수 있을 뿐만 아니라 결정을 설명할 수도 있으며, 이는 평가 결과를 감사하려는 경우 특히 유용할 수 있다. 아래 그림 3-7은 GPT-4가 판단을 설명하는 예시이다.

![image](https://github.com/user-attachments/assets/4f1d9b01-507b-47d6-b065-07bbc7548d17)

### AI를 심사위원으로 사용하는 방법

- AI를 사용하여 판단을 내리는 방법에는 여러 가지가 있다.
- 예를 들어, AI를 사용하여 응답 자체의 품질을 평가하거나, 해당 응답을 참조 데이터와 비교하거나, 해당 응답을 다른 응답과 비교할 수 있다.
- 위 세 가지 접근 방식에 대한 간단한 프롬프트 예시

![image](https://github.com/user-attachments/assets/14dc305b-4e5a-478a-bcb2-de1586307f65)

- 범용 AI 심사위원에게 어떤 기준에 따라 응답을 평가하도록 요청할 수 있음
- 역할극 챗봇을 구축하는 경우 챗봇의 응답이 사용자가 원하는 역할과 일치하는지 평가 가능
  - 홍보용 제품 사진을 생성하는 애플리케이션을 구축하는 경우 "이 이미지의 제품 신뢰도를 1점에서 5점까지 어떻게 평가하시겠습니까?"라고 물을 수 있음
- AI 심사위원 기준은 표준화되어 있지 않다
  - Azure AI Studio의 점수는 MLflow의 점수와 매우 다를 수 있으며, 이러한 점수는 심사위원의 기본 모델과 프롬프트에 따라 달라짐

![image](https://github.com/user-attachments/assets/2202f5a2-4a06-4ba2-9c6f-4b33a4841265)

- AI 심사위원에게 프롬프트를 제공하는 방법은 모든 AI 애플리케이션에 프롬프트를 제공하는 방법과 유사하며, 일반적으로 심사위원의 프롬프트는 다음을 명확하게 설명해야 함
  - 모델이 수행할 작업(예: 생성된 답변과 질문 간의 관련성 평가).
  - 모델이 평가를 위해 따라야 할 기준(예: "최우선 사항은 생성된 답변이 정답에 따라 주어진 질문을 해결하기에 충분한 정보를 포함하는지 여부를 결정하는 것이어야 한다"). 지침이 자세할수록 좋다.
  - 채점 시스템은 다음 중 하나일 수 있음
    - 분류(예: 좋음/나쁨 또는 관련 있음/관련 없음/중립).
    - 이산적인 숫자 값(예: 1에서 5). 이산적인 숫자 값은 각 클래스가 의미론적 해석 대신 숫자 해석을 갖는 분류의 특별한 경우로 간주될 수 있음
    - 연속적인 숫자 값(예: 0과 1 사이, 유사성 정도를 평가하려는 경우).
- 예제가 포함된 프롬프트가 더 나은 성능을 보인다는 것이 입증됨. 1점에서 5점 사이의 채점 시스템을 사용하는 경우 1, 2, 3, 4 또는 5점 응답이 어떤 모습인지, 가능하다면 특정 점수를 받은 이유에 대한 예를 포함하는 것이 좋다

> [!NOTE]
>
> 언어 모델은 일반적으로 숫자보다 텍스트에 더 능숙하여 AI 심사위원은 숫자 채점 시스템보다 분류에서 더 잘 작동한다고 보고됨.
> 숫자 채점 시스템의 경우 이산 채점이 연속 채점보다 더 잘 작동하는 것으로 보임. 경험적으로 이산 채점 범위가 넓을수록 모델 성능이 저하되는 경향이 있으며, 일반적인 이산 채점 시스템은 1에서 5 사이가 권장됨

![image](https://github.com/user-attachments/assets/23e39bc7-28ee-481e-8836-d57e9edf7d68)


> [!NOTE]
>
> - 아래는 Azure AI Studio에서 관련성 기준에 사용된 프롬프트의 일부임
> 귀하의 임무는 생성된 답변과 질문 간의 관련성을 정답을 기준으로 1점에서 5점 사이의 범위로 채점하고 채점 이유도 제공하는 것입니다.
> 주요 초점은 생성된 답변이 정답에 따라 주어진 질문을 해결하기에 충분한 정보를 포함하는지 여부를 결정하는 것이어야 합니다.
> ...
> 생성된 답변이 정답과 모순되면 낮은 점수(1-2점)를 받습니다.
> 예를 들어, "하늘이 파란가요?"라는 질문에 대한 정답은 "예, 하늘은 파랗습니다."이고 생성된 답변은 "아니요, 하늘은 파랗지 않습니다."입니다.
> 이 예에서 생성된 답변은 하늘이 파랗다는 사실에도 불구하고 하늘이 파랗지 않다고 진술함으로써 정답과 모순됩니다.
> 이러한 불일치는 낮은 점수(1-2점)를 초래하며 낮은 점수의 이유는 생성된 답변과 정답 간의 모순을 반영합니다.


### AI 심사위원의 한계

#### 불일치(Inconsistency)

- 평가 방법이 신뢰할 수 있으려면 결과가 일관되어야 함
- 그러나 모든 AI 애플리케이션과 마찬가지로 AI 심사위원도 확률적이며, 동일한 심사위원이 동일한 입력에 대해 프롬프트가 다르면 다른 점수를 출력할 수 있음
- 동일한 심사위원이라도 동일한 지침으로 프롬프트해도 두 번 실행하면 다른 점수를 출력할 수도 있으며, 이러한 불일치로 인해 평가 결과를 재현하거나 신뢰하기 어려움
- 2장에서 설명한 샘플링 변수를 사용하여 AI 심사위원을 더 일관되게 만들 수 있음
  - Zheng et al.(2023)은 프롬프트에 평가 예제를 포함하면 GPT-4의 일관성을 65%에서 77.5%로 높일 수 있음을 보여주었다.
  - 그러나 높은 일관성이 높은 정확성을 의미하지 않을 수 있다 : 심사위원이 일관되게 동일한 실수를 저지를 수 있음
  - 또한 더 많은 예제를 포함하면 프롬프트가 길어지고 긴 프롬프트는 추론 비용이 높아짐을 의미함 : Zheng et al.의 실험에서 프롬프트에 더 많은 예제를 포함하자 GPT-4 지출이 4배 증가

#### 기준 모호성(Criteria ambiguity)

- 인간이 만든 많은 지표와 달리 AI 심사위원 지표는 표준화되어 있지 않아 오해하고 오용하기 쉬움
- MLflow, Ragas, LlamaIndex는 모두 생성된 출력이 주어진 컨텍스트에 얼마나 충실한지를 측정하기 위한 기본 제공 기준 충실도(faithfulness)를 가지고 있지만 지침과 채점 시스템은 모두 다름
  - 아래 표에서 각각에 의해 산출된 점수를 비교할 수 있을까?

| 도구   | 프롬프트<br>[간결성을 위해 일부 생략]                                                                                                                                                                                                                                                                                                                           | 채점<br>시스템 |
| ------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------- |
| MLflow | 충실도는 제공된 출력과 제공된 컨텍스트로만 평가되며, 충실도를 채점할 때 제공된 입력을 완전히 무시하십시오. 충실도는 제공된 출력의 어느 정도가 제공된 컨텍스트와 사실적으로 일치하는지를 평가합니다....<br><br>충실도: 다음은 다양한 점수에 대한 세부 정보입니다.<br>- 점수 1: 출력의 주장 중 어느 것도 제공된 컨텍스트에서 추론할 수 없습니다.<br>- 점수 2: ... | 1–5            |
| Ragas  | 귀하의 임무는 주어진 컨텍스트를 기반으로 일련의 진술의 충실도를 판단하는 것입니다. 각 진술에 대해 컨텍스트를 기반으로 진술을 확인할 수 있으면 1로, 컨텍스트를 기반으로 진술을 확인할 수 없으면 0으로 평결을 반환해야 합니다.                                                                                                                                    | 0과 1          |
| LlamaIndex | 주어진 정보가 컨텍스트에 의해 지원되는지<br>알려주십시오.<br>YES 또는 NO로 답변해야 합니다.<br>대부분의 컨텍스트가 관련이 없더라도<br>컨텍스트 중 하나라도 정보를 지원하면 YES로 답변하십시오.<br>몇 가지 예가 아래에 제공됩니다.<br>정보: 사과 파이는 일반적으로 이중 껍질입니다.<br>컨텍스트: 사과 파이는 과일 파이입니다… 일반적으로 이중<br>껍질이며,<br>페이스트리가<br>필링 위아래에 모두 있습니다.<br>답변: YES | YES와<br>NO    |

- 애플리케이션은 시간이 지남에 따라 진화하지만, 평가 방식은 이상적으로는 고정되어야 함.
  - 이렇게 하면 평가 지표를 사용하여 애플리케이션의 변경 사항을 모니터링할 수 있음
  - 그러나 AI 심사위원도 AI 애플리케이션이므로 시간이 지남에 따라 변경될 수 있다
  - 지난달 애플리케이션의 일관성 점수가 90%였고 이번 달에는 92%가 나왔다면, 이는 일관성이 향상되었다는 의미일까?
  - 프롬프트의 수정만으로도 기존과 다른 점수가 출력될 수 있음
  - *심사위원에게 사용된 모델과 프롬프트를 볼 수 없다면 어떤 AI 심사위원도 신뢰하지 마십시오.*
 
#### 비용 및 대기 시간 증가

- 많은 경우 위험을 줄이기 위해 운영 환경에서 AI 심사위원을 안전 장치로 사용하며, AI 심사위원이 좋다고 판단한 생성된 응답만 사용자에게 보여줌
- 강력한 모델을 사용하여 응답을 평가하는 것은 비용이 많이 들 수 있다
  - GPT-4를 사용하여 응답을 생성하고 평가하면 GPT-4 호출 수가 두 배가 되어 API 비용이 거의 두 배가 됨
  - 세 가지 기준(예: 전체 응답 품질, 사실적 일관성, 유해성)을 평가하기 위해 세 가지 평가 프롬프트가 있는 경우 API 호출 수가 네 배로 증가
- 약한 모델을 심사위원으로 사용하여 비용을 줄일 수 있다
- 또한 응답의 하위 집합만 평가하는 현장 점검(spot-checking)으로 비용을 줄일 수 있다
  - 현장 점검은 일부 실패를 포착하지 못할 수 있음을 의미하며, 평가하는 샘플 비율이 클수록 평가 결과에 대한 신뢰도가 높아지지만 비용도 높아짐
- 프로덕션 파이프라인에 AI 심사위원을 구현하면 대기 시간이 추가되며, 사용자에게 응답을 반환하기 전에 응답을 평가하면 위험은 줄어들지만 대기 시간은 증가하는 절충안에 직면

#### AI 심사위원의 편향

- 인간 평가자와 마찬가지로 AI 심사위원도 편향(bias)이 존재할 수 있음
- AI 심사위원은 모델이 다른 모델에서 생성된 응답보다 자체 응답을 선호하는 자기 편향(self-bias)을 갖는 경향이 있다
  - Zheng et al.의 2023년 실험에서 GPT-4는 10% 더 높은 승률로 자신을 선호했고, Claude-v1은 25% 더 높은 승률로 자신을 선호하였음
- 많은 AI 모델은 첫 번째 위치 편향을 가지고 있으며, AI 심사위원은 쌍으로 비교하거나 옵션 목록에서 첫 번째 답변을 선호할 수 있다.
  - 이는 다른 순서로 동일한 테스트를 여러 번 반복하거나 신중하게 작성된 프롬프트를 사용하여 완화 가능함
  - AI의 위치 편향은 인간의 위치 편향과 반대 : 인간은 마지막으로 본 답변을 선호하는 경향이 있으며, 이를 최신성 편향(recency bias)이라고 한다
- 일부 AI 심사위원은 품질에 관계없이 더 긴 답변을 선호하는 장황함 편향(verbosity bias)을 가지고 있다
  - Wu and Aji(2023)는 GPT-4와 Claude-1 모두 사실적 오류가 있는 더 긴 응답(약 100단어)을 더 짧고 정확한 응답(약 50단어)보다 선호한다는 것을 발견
  - Saito et al.(2023)은 창의적인 작업에 대한 이러한 편향을 연구했으며 길이 차이가 충분히 클 때(예: 한 응답이 다른 응답보다 두 배 길 때) 심사위원이 거의 항상 더 긴 응답을 선호한다는 것을 발견
  - Zheng et al.(2023)과 Saito et al.(2023) 모두 GPT-4가 GPT-3.5보다 이 편향에 덜 취약하다는 것을 발견했으며, 이는 모델이 더 강력해짐에 따라 이 편향이 사라질 수 있음을 시사
- AI 심사위원은 개인 정보 보호 및 IP를 포함한 모든 AI 애플리케이션과 동일한 한계를 가지고 있음
  - 독점 모델을 심사위원으로 사용하는 경우 이 모델에 데이터를 보내야 하며, 모델 제공업체가 훈련 데이터를 공개하지 않으면 심사위원이 상업적으로 사용하기에 안전한지 확실히 알 수 없음

### 어떤 모델이 심사위원 역할을 할 수 있는가?

- 심사위원은 심사 대상 모델보다 강하거나, 약하거나, 동일할 수 있으며,  각 시나리오에는 장단점이 존재
  
- 강한 심사위원 : 시험 채점관이 시험 응시자보다 더 많은 지식을 가져야 하지 않을까?
  - 더 강력한 모델을 사용가능한데 굳이 심사에만 쓰고 응답 생성에는 사용하지 않는 이유 : 비용과 대기 시간
  - 모든 응답을 생성하기 위해 더 강력한 모델을 사용할 예산이 없을 수 있으므로 응답의 하위 집합을 평가하는 데 사용
  - 예를 들어, 저렴한 사내 모델을 사용하여 응답을 생성하고 GPT-4를 사용하여 응답의 1%를 평가할 수 있다
  - 또한, 빠른 모델을 사용하여 응답을 생성하는 동안 더 강력하지만 느린 모델이 백그라운드에서 평가를 수행할 수 있음
  - 강력한 모델이 약한 모델의 응답이 나쁘다고 생각하면 강력한 모델의 응답으로 응답을 업데이트하는 등 개선 조치 가능
  - 이 반대의 예시도 가능하다 : 강한 응답 모델 & 약한 심사위원
  - 더 강력한 모델을 심사위원으로 사용하는 방식의 2가지 과제
    - 첫째, 가장 강력한 모델은 적격한 심사위원이 없음
    - 둘째, 어떤 모델이 가장 강력한지 결정하기 위한 대안적인 평가 방법이 필요

- 동일한 심사위원 : 모델 스스로가 자신을 평가하도록 설계
- 자기 평가(self-evaluation) 또는 *자기 비판(self-critique)*은 특히 자기 편향 때문에 부정 행위처럼 들릴 수 있으나, 자기 평가는 건전성 검사에 매우 유용할 수 있음
- 건전성 검사 외에도 모델에게 자신을 평가하도록 요청하면 모델이 응답을 수정하고 개선하도록 유도할 수 있다

- 약한 심사위원 : 심사위원이 심사 대상 모델보다 약할 수 있는가?
- 일부는 판단이 생성보다 쉬운 작업이라고 주장함 : 누구나 노래가 좋은지 아닌지에 대한 의견을 가질 수 있지만 모든 사람이 노래를 쓸 수 있는 것은 아니다
- 범용 심사위원의 경우 더 강력한 모델을 사용하는 것이 인간 선호도와 더 가까울 수 있음
- 특화된 심사위원은 특정 기준을 사용하고 특정 채점 시스템을 따르면서 특정 판단을 내리도록 훈련
- 작고 특화된 심사위원은 특정 판단에 대해 더 크고 범용적인 심사위원보다 더 신뢰할 수 있다
- 특화된 AI 심사위원도 여러 가지가 있을 수 있음
  - 보상 모델 : (프롬프트, 응답) 쌍을 입력으로 받아 프롬프트가 주어졌을 때 응답이 얼마나 좋은지 점수를 매김. RLHF에서 성공적으로 사용. Cappy가 대표적
  - 참조 기반 심사위원 : 하나 이상의 참조 응답과 관련하여 생성된 응답을 평가하여 유사도 점수 또는 품질 점수(참조 응답과 비교하여 생성된 응답이 얼마나 좋은지)를 출력. BLEURT, Prometheus 등
  - 선호도 모델 : (프롬프트, 응답 1, 응답 2)를 입력으로 받아 주어진 프롬프트에 대해 두 응답 중 어느 것이 더 나은지(사용자가 선호하는지) 출력. PandaLM, JudgeLM 등
  - 선호도 데이터는 AI 모델을 인간 선호도에 맞추는 데 필수적이며 수집하기 어렵고 비용이 많이 듦
  - 좋은 인간 선호도 예측기를 갖는 것은 일반적으로 평가를 더 쉽게 만들고 모델을 더 안전하게 사용할 수 있게 함
 
![image](https://github.com/user-attachments/assets/f45905a6-c8f6-49df-a5fd-2b6492d4d432)

## 비교 평가를 통한 모델 순위 지정

- 어떤 모델이 자신에게 가장 적합한지 알기 위해 모델에 대한 평가가 필요
- 점수별 평가 또는 비교 평가를 사용하여 모델 순위를 지정할 수 있음
- 점수별 평가를 사용하면 각 모델을 독립적으로 평가한 다음 점수로 순위를 매김 : 어떤 댄서가 최고인지 알고 싶다면, 각 댄서를 개별적으로 평가하고 점수를 매긴 다음 가장 높은 점수를 받은 댄서를 선택
- 비교 평가를 사용하면 모델을 서로 비교하여 평가하고 비교 결과에서 순위를 계산 : 댄스 경연 대회에서 모든 후보에게 나란히 춤을 추도록 요청하고 대부분의 심사위원이 선호하는 댄서를 선택
- 품질이 주관적인 응답의 경우 비교 평가는 일반적으로 점수별 평가보다 수행하기 쉽다
  - 예를 들어, 두 곡 중 어느 곡이 더 나은지 판단하는 것이 각 곡에 구체적인 점수를 주는 것보다 쉬움
- AI에서 비교 평가는 2021년 Anthropic이 다양한 모델의 순위를 매기는 데 처음 사용하였으며, 또한 커뮤니티의 쌍별 모델 비교에서 계산된 점수를 사용하여 모델 순위를 매기는 인기 있는 LMSYS의 Chatbot Arena 리더보드를 지원
- 아래 그림과 같이 많은 모델 제공업체가 운영에서 모델을 평가하기 위해 비교 평가를 사용함. 이러한 출력은 서로 다른 모델 또는 서로 다른 샘플링 변수를 가진 동일한 모델에 의해 생성될 수 있음

![image](https://github.com/user-attachments/assets/9f30f77d-ca78-43c3-a752-7dcbed1a6385)

- 사용자에게 선택하도록 요청하는 것도 사용자 불만을 유발할 수 있다
  - 답을 모르기 때문에 모델에게 수학 문제를 물었는데 모델이 두 가지 다른 답을 주고 어느 것을 선호하는지 선택하도록 요청한다?
- 사용자로부터 비교 피드백을 수집할 때 한 가지 과제는 어떤 질문이 선호도 투표로 결정될 수 있고 어떤 질문은 그렇지 않은지 결정하는 것
- 선호도 기반 투표는 투표자가 해당 주제에 대해 잘 알고 있는 경우에만 작동함
  - 이 접근 방식은 일반적으로 AI가 인턴이나 조수 역할을 하여 사용자가 수행 방법을 아는 작업을 가속화하는 데 도움이 되는 애플리케이션에서 작동
  - 사용자가 AI에게 자신이 수행 방법을 모르는 작업을 수행하도록 요청하는 곳에서는 작동하지 않음
- 모델 비교 평가 예시 : 비교 결과가 주어지면 평가 알고리즘(rating algorithm)을 사용하여 모델 순위를 계산
  - 일반적으로 이 알고리즘은 먼저 비교 신호에서 각 모델의 점수를 계산한 다음 점수로 모델 순위를 매김
  - Elo, Bradley-Terry, TrueSkill과 같이 이러한 다른 도메인을 위해 개발된 많은 평가 알고리즘을 AI 모델 평가에 적용할 수 있음

![image](https://github.com/user-attachments/assets/238c7317-faa6-4bdb-b091-8a8d7440faba)

### 비교 평가의 과제

#### 확장성 병목 현상(Scalability bottlenecks)

- 비교 평가는 데이터 집약적이며, 비교할 모델 쌍의 수는 모델 수에 따라 2차적으로 증가
  - 2024년 1월 LMSYS는 244,000개의 비교를 사용하여 57개의 모델을 평가
  - 전체로 보면 많아 보이나 모델 쌍당 평균 153개의 비교에 불과(57개 모델은 1,596개의 모델 쌍에 해당). 이는 파운데이션 모델이 수행하기를 원하는 광범위한 작업을 고려할 때 적은 수임
- 다행히 어떤 모델이 더 나은지 결정하기 위해 항상 두 모델 간의 직접적인 비교가 필요한 것은 아님
  - 순위 알고리즘은 일반적으로 전이성(transitivity)을 가정
  - 모델 A가 B보다 순위가 높고 B가 C보다 순위가 높으면 전이성을 통해 A가 C보다 순위가 높다고 추론 가능
  - 그러나 전이성 가정에도 한계가 존재함 : 인간의 선호도가 반드시 전이적이지 않으며, 또한 서로 다른 모델 쌍이 서로 다른 평가자에 의해 서로 다른 프롬프트에서 평가되기 때문에 비전이성이 발생할 수 있음
- 새로운 모델을 평가하는 것도 어려운 과제
  - 독립적인 평가를 사용하면 새로운 모델만 평가하면 됨
  - 비교 평가를 사용하면 새로운 모델을 기존 모델과 비교하여 평가해야 하며, 이는 기존 모델의 순위를 변경할 수 있음
  - 이는 비공개 모델의 평가를 특히 어렵게 함 : 내부 데이터를 사용하여 회사용 모델을 구축한 경우, 자체적으로 비교 평가를 수행하거나 공개 리더보드 중 하나에 비용을 지불하여 비공개 평가를 실행해야 함
- 확장성 병목 현상은 매칭 알고리즘의 고도화를 통해 해결 가능 : 이미 10 대 0인 매칭을 계속 실험할 필요는 없다

#### 표준화 및 품질 관리 부족

- 비교 신호를 수집하는 한 가지 방법은 LMSYS Chatbot Arena가 하는 것처럼 커뮤니티에 비교를 크라우드소싱
- 이 접근 방식의 이점은 광범위한 신호를 포착하고 비교적 결과를 조작하기 어렵다는 것
- 그러나 단점은 표준화 및 품질 관리를 시행하기 어렵다
  - 인터넷 접속이 가능한 누구나 이러한 모델을 평가하기 위해 어떤 프롬프트든 사용할 수 있으며, 더 나은 응답을 구성하는 요소에 대한 표준은 없음
    - 자원봉사자에게 응답을 사실 확인하도록 기대하는 것은 무리일 수 있으므로, 그들은 더 좋게 들리지만 사실적으로 부정확한 응답을 무심코 선호할 수 있다
    - 어떤 사람들은 예의 바르고 온건한 응답을 선호하는 반면, 다른 사람들은 필터 없는 응답을 선호할 수 있다
  - 비교를 크라우드소싱하려면 사용자가 작업 환경 외부에서 모델을 평가해야 함
    - 실제 환경에 기반하지 않으면 테스트 프롬프트가 실제 환경에서 이러한 모델이 사용되는 방식을 반영하지 못할 수 있다
    - 사람들은 그냥 생각나는 첫 번째 프롬프트를 사용하고 정교한 프롬프팅 기술을 사용할 가능성은 낮음
    - 2023년 LMSYS Chatbot Arena에서 발표한 33,000개의 프롬프트 중 180개는 "hello"와 "hi"이며, 이는 데이터의 0.55%를 차지하며, 이는 아직 "hello!", "hello.", "hola", "hey" 등과 같은 변형을 포함하지도 않음
    - 너무 많은 간단한 프롬프트를 사용하여 모델을 평가하면 순위가 오염될 수 있다
- 표준화를 시행하는 한 가지 잠재적인 방법은 사용자를 미리 정해진 프롬프트 세트로 제한하는 것
  - 그러나 이는 리더보드가 다양한 사용 사례를 포착하는 능력에 영향을 미칠 수 있음
  - 대신 LMSYS는 사용자가 어떤 프롬프트든 사용하도록 허용하지만 내부 모델을 사용하여 어려운 프롬프트를 필터링하고 이러한 어려운 프롬프트만 사용하여 모델 순위를 매김
- 또 다른 방법은 신뢰할 수 있는 평가자만 사용하는 것
  - 두 응답을 비교하는 기준에 대해 평가자를 훈련하거나 실용적인 프롬프트와 정교한 프롬프팅 기술을 사용하도록 훈련할 수 있다
  - 이 접근 방식의 단점은 비용이 많이 들고 얻을 수 있는 비교 수가 크게 줄어들 수 있다는 것
- 또 다른 옵션은 비교 평가를 제품에 통합하고 사용자가 워크플로 중에 모델을 평가하도록 하는 것
  - 코드 생성 작업의 경우 사용자 코드 편집기 내에서 사용자에게 두 가지 코드 스니펫을 제안하고 더 나은 것을 선택하도록 할 수 있음
  - 많은 애플리케이션이 이미 이 작업을 수행하고 있으나, 이전에 언급했듯이 사용자는 전문가가 아니므로 어떤 코드 스니펫이 더 나은지 모를 수 있다
- 게다가 사용자는 두 가지 옵션을 모두 읽지 않고 무작위로 하나를 클릭할 수 있으며, 이는 결과에 많은 노이즈를 유발할 수 있음
- 그러나 올바르게 투표하는 소수의 사용자로부터 얻은 신호는 어떤 모델이 더 나은지 결정하는 데 충분할 수 있음
- *일부 팀은 인간 평가자보다 AI를 선호함. AI는 훈련된 인간 전문가만큼 좋지는 않지만 무작위 인터넷 사용자보다 더 신뢰할 수 있을 수 있다*

#### 비교 성능에서 절대 성능으로

- 비교 평가는 어떤 모델이 더 나은지 알려줄 수 있으나, 모델이 얼마나 좋은지 또는 이 모델이 사용 사례에 충분히 좋은지 여부는 알려주지 않는다
  - 비교 평가 결과 모델 B가 모델 A보다 낫다면, 다음 시나리오 중 어느 것이든 유효할 수 있음
    - 모델 B는 좋지만 모델 A는 나쁩니다.
    - 모델 A와 모델 B 모두 나쁩니다.
    - 모델 A와 모델 B 모두 좋습니다.
- 고객 지원에 모델 A를 사용하고 있으며 모델 A가 전체 티켓의 70%를 해결할 수 있다고 가정
  - 모델 B와 비교 평가 결과 A에 대해 51%의 승률을 보임
  - 이 51%의 승률이 모델 B가 해결할 수 있는 요청 수로 어떻게 전환될지는 불분명함
  - 여러 사람들이 자신의 경험에 따르면 승률 1% 변화가 일부 애플리케이션에서는 엄청난 성능 향상을 유발할 수 있지만 다른 애플리케이션에서는 최소한의 향상만 유발한다고 언급
- A를 B로 교체하기로 결정할 때 인간의 선호도 뿐만 아니라 비용과 같은 다른 요소도 고려해야함
  - 어떤 성능 향상을 기대해야 할지 모르면 비용-편익 분석을 하기가 어려움
  - 모델 B의 비용이 A의 두 배라면 비교 평가는 B의 성능 향상이 추가 비용을 감수할 가치가 있는지 판단하는 데 충분하지 않다
 
### 비교 평가의 미래

- 비교 평가의 여러 단점에도 불구하고 여전히 비교 평가가 갖는 장점이 명확함
  - 사람들은 각 출력에 구체적인 점수를 주는 것보다 두 출력을 비교하는 것을 더 쉽게 느끼며, 모델이 더 강력해지고 인간의 성능을 능가함에 따라 인간 평가자가 모델 응답에 구체적인 점수를 주는 것이 불가능할 수 있다
  - 비교 평가는 우리가 관심을 갖는 품질인 인간 선호도를 포착하는 것을 목표로 하며, 고정된 평가 벤치 마크에 비해 비교 평가는 결코 포화 상태가 되지 않음
  - 비교 평가는 참조 데이터로 모델을 훈련하는 것과 같이 쉽게 속일 수 있는 방법이 없으므로 비교적 조작하기 어렵고, 이 때문에 많은 사람들이 비교 평가 결과를 더 신뢰함

